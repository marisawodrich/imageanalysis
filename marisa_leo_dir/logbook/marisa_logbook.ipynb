{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1599a432",
      "metadata": {
        "id": "1599a432"
      },
      "source": [
        "# Logbook\n",
        "### Marisa Wodrich\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCch-glc31Ru"
      },
      "source": [
        "\n",
        "#### Date: 28.03.2022\n",
        "\n",
        "#### Project: Image analysis for high-throughput microscopy screening\n",
        "\n",
        "#### Aim: light introduction to instance segmentation\n",
        "\n",
        "#### Stepwise description of work:\n",
        "* recap what instance segmentation was and methods that are commonly used\n",
        "* talk to some people from my previous university that have worked with U-Nets before\n",
        "\n",
        "#### Result/Conclusion: \n",
        "* In instance segmentation we want to detect objects in an image and find their shapes and borders. For very simple object, one may use thresholding for binary segmentation or some a bit more advanced methods like watershed. If we would know the shape or some approximate of it, we could also use the hit-or-miss operator from morphology or alike. In our case, it is however a lot more complicated. Cells are not that easily detected with such operations and even if, we would still not be able to classify them. There are however some ANN structures that can solve both of the task of segmenting and classifying the objects (cells in our case).  \n",
        "* I talked to people from UOS that have worked with U-Nets before and got a brief understanding. It was not really in depth and I will need to read up all the details.\n",
        "\n",
        "#### Next steps: \n",
        "* Setting up my computer for this project \n",
        "* Looking into Malou's thesis and specifically the U-Net structure she used for her network\n",
        "*** "
      ],
      "id": "KCch-glc31Ru"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfqwuEnU19If"
      },
      "source": [
        "\n",
        "#### Date: 29.03.\n",
        "\n",
        "#### Project: Image analysis for high-throughput microscopy screening\n",
        "\n",
        "#### Aim: Setting up my computer for the project\n",
        "\n",
        "#### Stepwise description of work:\n",
        "* clone the repository\n",
        "* install tensorflow on my windows\n",
        "* install vs code on my windows\n",
        "\n",
        "#### Result/Conclusion: \n",
        "Unfortunatly I destroyed my ubuntu and until I have time to fix it, I need to work under windows. I installed git and cloned the repository. Then I also installed tensorflow and vs code for coding (in case we don't use colab).\n",
        "\n",
        "#### Next steps: \n",
        "* recap my DL knowledge and tensorflow skills\n",
        "* read Malou's report\n",
        "* install cvat\n",
        "*** "
      ],
      "id": "UfqwuEnU19If"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4fM6b7VzZFW"
      },
      "source": [
        "\n",
        "#### Date: 31.03.2022\n",
        "\n",
        "#### Project: Image analysis for high-throughput microscopy screening\n",
        "\n",
        "#### Aim: Recap tensorflow skills and start looking at Malou's thesis.\n",
        "\n",
        "#### Stepwise description of work:\n",
        "* Looking at some of my old projects in tensorflow  \n",
        "* start reading Malou's thesis\n",
        "\n",
        "#### Result/Conclusion: \n",
        "* I refreshed my tensorflow knowledge. My old projects can all be found on my Github and some of them are also public. We might be able to reuse some of my old code for the project, specifically my implementation of an autoencoder and its training might be useful (https://github.com/marisawodrich/tf-homework-2020/blob/main/Homework07.ipynb).\n",
        "* Reading the thesis is still ongoing.\n",
        "\n",
        "#### Next steps: \n",
        "* Continue with Malou's thesis\n",
        "* install the annotation tool cvat\n",
        "*** "
      ],
      "id": "f4fM6b7VzZFW"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsuObfjtlVeV"
      },
      "source": [
        "\n",
        "#### Date: 04.04.2022\n",
        "\n",
        "#### Project: Image analysis for high-throughput microscopy screening\n",
        "\n",
        "#### Aim: Reading through Malou's thesis. Installing cvat\n",
        "\n",
        "#### Stepwise description of work:\n",
        "* Read the thesis\n",
        "* Trying to install cvat\n",
        "\n",
        "#### Result/Conclusion: \n",
        "* Malou used a U-net architecture of encoder-decoder structure. Figure 4 in her thesis is good to understand the model structure.\n",
        "* I could not install cvat on my windows. I ran into docker problems and both Leo and me could not fix it. We decided together that we will both annotate images on his laptop, as he managed to successfully install cvat.\n",
        "\n",
        "\n",
        "#### Next steps: \n",
        "* Understanding the HoVer-Net architecture and why it might be an improvement to the U-Net\n",
        "*** "
      ],
      "id": "VsuObfjtlVeV"
    },
    {
      "cell_type": "markdown",
      "id": "c4e2c031",
      "metadata": {
        "id": "c4e2c031"
      },
      "source": [
        "\n",
        "#### Date: 05.04.2022\n",
        "\n",
        "#### Project: Image analysis for high-throughput microscopy screening\n",
        "\n",
        "#### Aim: Understanding the HoVer-Net architecture\n",
        "\n",
        "#### Stepwise description of work:\n",
        "* Reading the HoVer-Net Paper\n",
        "* Watching a video about the HoVer-Net and its application purposed (https://www.youtube.com/watch?v=PfDauqZdUCE)\n",
        "* Looking at the HoVer-Net code\n",
        "* Going through some of my old tensorflow projects together with Leo\n",
        "\n",
        "#### Result/Conclusion: \n",
        "The HoVer-Net is an CNN-based architecture specifically designed for instance segmentation in medical cell images. The model is split into different parts and is generally based on an autoencoder model structure. For the encoder part, a pretrained ResNet50 is used. Instead of one simple decoder, the HoVer-Net is split into 3 different decoder branches (all could be seen as stand-alone models). Each branch has a different purpose. The Nuclear Pixel (NP) branch detects the cell blobs and outputs a matrix with zeros for non-cell-belonging pixels and a unique number for each connected region that was predicted to be a cell. The HoVer branch has the goal to predict vertical and horizontal maps of the cells. This is particularly useful (and a new extension to all previously existing models!) in order to seperate detected cells. In instance segmentation and specifically in cell segmentation, this is very useful. Cells tend to be very close to each other and might even overlap. If we consider both the vertical and the horizontal maps, we can seperate cells that were one blob before. (One thought for the future: Could it improve if we would also include the two diagonal maps?). Already alone with these two branches (NP and HoVer), one can perform the cell segmentation. The third branch is optional to use, depending on which labels we have during training. It is called the Nuclear Classification (NC) branch and performs the task of predicting the cell type label. \n",
        "\n",
        "#### Next steps: \n",
        "* Understand the data (mostly the labels and output that the network should predict. The input is obviously just an image)\n",
        "* figure out how to try out and run the published HoVer-Net code (https://github.com/vqdang/hover_net/)\n",
        "\n",
        "*** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qYOQgSllVQX"
      },
      "source": [
        "\n",
        "#### Date: 06.04.2022\n",
        "\n",
        "#### Project: Image analysis for high-throughput microscopy screening\n",
        "\n",
        "#### Aim: Understanding the HoVer-Net code and data\n",
        "\n",
        "#### Stepwise description of work:\n",
        "* Downloading the data and looking at the structure. Understanding the labels.\n",
        "* Looking into the published HoVer-Net code (the tensorflow version. This is also what they used for their paper)\n",
        "* Understanding the file-structure and finding out which are the essential parts we need\n",
        "* Setting up a notebook for trying out the code\n",
        "\n",
        "#### Result/Conclusion: \n",
        "* Regarding the data, we have RGB-images of size 1000x1000x3 as input data. For the labels, we have different things (all stored as matlab data). For the first branch (NP), we have a 1000x1000 matrix which contains zeros for all pixels not belonging to cells a number for all other. Each connected region has a unique value. For the second branch (HoVer), we have a list of tuples, which contains the x and y value of the centroid location for all of the cells. (From the centroid, one can calculate the horizontal and vertical maps easily, if I understand it correctly). For the third branch (NC), we have a list containing the class label for each cell, as well as 1000x1000 map where each pixel has the corresponding label (0 for no cell, 1-4 for the different cell types (epithelial, inflammatory, spindle-shaped, miscellaneous)). \n",
        "* The code is a mess. There is a version in tensorflow, which is also described in the paper, as well as a version in pytorch. Leo and I decided to work with tensorflow. The code is weirdly nested and there are many many files and very very limited comments or descriptions. It seems like they use some outdated tensorflow version, or at least some outdated functions. A lot of things need to be adapted in order to get the code to work. \n",
        "* We tried to set up an environment to run the code locally and failed at doing so. On Leo's laptop, there was some tensorflow problem, which made it impossible to use it. We could not debug it. While my laptop does have a graphical card, I unfortunatly just broke my ubuntu and can't get it to work under windows. We therefore decided to switch to colab... \n",
        "* I started a notebook in colab to try out the existing code. This is very painful because of all the inter-file dependencies in the original python files. There is a lot of debugging and adapting code to up-to-date ANN style and tf functions. The code is unfortunately still not working.\n",
        "* Open questions I have: To me it seems like the second branch can simply be calculated from the result of the first branch. I don't know why we would need a complex ANN for this. It's quite trivial if we have already segmented the instances. The third branch seems to make most sense if we would pass the output from the second branch as input. This all suggests that the models are linearly connected and not in parallel as the graphics from the paper suggest. I should investigate this further by looking into how the network is trained and which branch gets which input. If the branches are indeed independent like the graphic suggests, then it would be redundant to have the first two, as all information is contained in the output of branch 3. \n",
        "\n",
        "#### Next steps: \n",
        "* getting the code to work\n",
        "* coding my own version of the HoVer-Net in my own coding style in colab\n",
        "*** "
      ],
      "id": "1qYOQgSllVQX"
    },
    {
      "cell_type": "markdown",
      "source": [
        "*template*"
      ],
      "metadata": {
        "id": "3uRLA5rA6h_S"
      },
      "id": "3uRLA5rA6h_S"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Us6oH3FplW1g"
      },
      "source": [
        "\n",
        "#### Date:\n",
        "\n",
        "#### Project: Image analysis for high-throughput microscopy screening\n",
        "\n",
        "#### Aim:\n",
        "\n",
        "#### Stepwise description of work:\n",
        "\n",
        "#### Result/Conclusion: \n",
        "\n",
        "#### Next steps: \n",
        "*** "
      ],
      "id": "Us6oH3FplW1g"
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vMYjFIWg16Zv"
      },
      "id": "vMYjFIWg16Zv",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "marisa_logbook.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}